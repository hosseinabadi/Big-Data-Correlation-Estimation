{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from io import BytesIO\n",
    "import polars as pl\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltat = 5\n",
    "\n",
    "assets_tickers = [\"EUSA\", \"EWC\", \"EWU\", \"EWG\", \"EWQ\", \"EWJ\", \"MCHI\", \"INDA\", \"EWA\", \"EWZ\", \"EWY\", \"EWW\", \"EWL\", \"EWT\", \"EWH\", \"EWS\", \"EWI\", \"EWP\", \"EWN\", \"EWD\", \"EWO\", \"EWK\", \"EDEN\", \"EFNL\", \"EIS\"]\n",
    "\n",
    "# List of yearly `.tar` files to process\n",
    "yearly_tar_files = [\n",
    "    \"Data/ETFs/ETFs-2009.tar\",\n",
    "    \"Data/ETFs/ETFs-2010.tar\",\n",
    "    \"Data/ETFs/ETFs-2011.tar\",\n",
    "    \"Data/ETFs/ETFs-2012.tar\",\n",
    "]\n",
    "average_diffs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_timeseries(data):\n",
    "\n",
    "    base_date = datetime(1899, 12, 30)\n",
    "\n",
    "    data = data.with_columns(\n",
    "        (base_date + pl.col(\"xltime\").cast(float) * timedelta(days=1)).alias(\"datetime\")\n",
    "    )\n",
    "\n",
    "    data = data.with_columns(pl.col(\"datetime\").dt.convert_time_zone(\"America/New_York\"))\n",
    "\n",
    "    data = data.drop(\"xltime\").sort(\"datetime\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Define the expected schemas\n",
    "schemas = {\n",
    "    \"bbo\": {\n",
    "        \"xltime\": pl.Float64,\n",
    "        \"bid-price\": pl.Float64,\n",
    "        \"bid-volume\": pl.Int32,\n",
    "        \"ask-price\": pl.Float64,\n",
    "        \"ask-volume\": pl.Int32,\n",
    "    },\n",
    "    \"trade\": {\n",
    "        \"xltime\": pl.Float64,\n",
    "        \"trade-price\": pl.Float64,\n",
    "        \"trade-volume\": pl.Int32,\n",
    "        \"trade-stringflag\": pl.Utf8,\n",
    "        \"trade-rawflag\": pl.Utf8,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary to store concatenated DataFrames for each asset\n",
    "assets_data = {}\n",
    "\n",
    "def validate_and_fix_schema(df, expected_schema):\n",
    "    for col, expected_type in expected_schema.items():\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing column: {col}\")\n",
    "        \n",
    "        # If the column type does not match the expected type\n",
    "        if df[col].dtype != expected_type:\n",
    "            # Handle Float64 columns\n",
    "            if expected_type == pl.Float64:\n",
    "                df = df.with_columns(\n",
    "                    # Ensure the column is cast to Utf8 for string operations\n",
    "                    pl.when(pl.col(col).cast(pl.Utf8).str.strip_chars().is_in([\"\", \"()\", None]))\n",
    "                    .then(None)\n",
    "                    .otherwise(pl.col(col).cast(pl.Utf8))\n",
    "                    .str.replace_all(r\"[^\\d.]\", \"\")  # Remove non-numeric characters\n",
    "                    .cast(pl.Float64)  # Cast back to Float64\n",
    "                    .alias(col)\n",
    "                )\n",
    "            # Handle Int32 columns\n",
    "            elif expected_type == pl.Int32:\n",
    "                df = df.with_columns(\n",
    "                    # Ensure the column is cast to Utf8 for string operations\n",
    "                    pl.when(pl.col(col).cast(pl.Utf8).str.strip_chars().is_in([\"\", \"()\", None]))\n",
    "                    .then(None)\n",
    "                    .otherwise(pl.col(col).cast(pl.Utf8))\n",
    "                    .str.replace_all(r\"[^\\d]\", \"\")  # Remove non-numeric characters\n",
    "                    .cast(pl.Int32)  # Cast back to Int32\n",
    "                    .alias(col)\n",
    "                )\n",
    "            else:\n",
    "                # Cast other types directly\n",
    "                df = df.with_columns(df[col].cast(expected_type).alias(col))\n",
    "    return df\n",
    "\n",
    "for sector in tqdm(assets_tickers):\n",
    "    del assets_data\n",
    "    del schemas \n",
    "    schemas = {\n",
    "        \"bbo\": {\n",
    "            \"xltime\": pl.Float64,\n",
    "            \"bid-price\": pl.Float64,\n",
    "            \"bid-volume\": pl.Int32,\n",
    "            \"ask-price\": pl.Float64,\n",
    "            \"ask-volume\": pl.Int32,\n",
    "        },\n",
    "        \"trade\": {\n",
    "            \"xltime\": pl.Float64,\n",
    "            \"trade-price\": pl.Float64,\n",
    "            \"trade-volume\": pl.Int32,\n",
    "            \"trade-stringflag\": pl.Utf8,\n",
    "            \"trade-rawflag\": pl.Utf8,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    assets_data = {}\n",
    "    # Step 1: Iterate through yearly `.tar` files\n",
    "    for yearly_tar_path in yearly_tar_files:\n",
    "        print(f\"Processing yearly tar: {yearly_tar_path}\")\n",
    "        \n",
    "        with tarfile.open(yearly_tar_path, \"r\") as outer_tar:\n",
    "            # Step 2: Iterate through files in the yearly `.tar`\n",
    "            for member in outer_tar.getmembers():\n",
    "                if member.isfile() and member.name.startswith(f\"./{sector}\") and member.name.endswith(\".tar\"):\n",
    "                    print(f\"Processing inner tar: {member.name}\")\n",
    "                    \n",
    "                    # Determine file type (\"bbo\" or \"trade\") based on the name\n",
    "                    if \"bbo\" in member.name:\n",
    "                        file_type = \"bbo\"\n",
    "                        expected_schema = schemas[\"bbo\"]\n",
    "                    elif \"trade\" in member.name:\n",
    "                        continue\n",
    "                        file_type = \"trade\"\n",
    "                        expected_schema = schemas[\"trade\"]\n",
    "                    else:\n",
    "                        print(f\"Skipping unknown file type: {member.name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Step 3: Extract the inner `.tar` file\n",
    "                    inner_tar_data = BytesIO(outer_tar.extractfile(member).read())\n",
    "                    with tarfile.open(fileobj=inner_tar_data, mode=\"r\") as inner_tar:\n",
    "                        parquet_files = []\n",
    "                        \n",
    "                        for inner_member in inner_tar.getmembers():\n",
    "                            # Look for `.parquet` files\n",
    "                            if inner_member.isfile() and inner_member.name.endswith(\".parquet\"):\n",
    "                                parquet_data = BytesIO(inner_tar.extractfile(inner_member).read())\n",
    "                                df = pl.read_parquet(parquet_data)\n",
    "                                \n",
    "                                # Validate and fix schema\n",
    "                                try:\n",
    "                                    df = validate_and_fix_schema(df, expected_schema)\n",
    "                                    parquet_files.append(df)\n",
    "                                except ValueError as e:\n",
    "                                    print(f\"Schema error in file {inner_member.name}: {e}\")\n",
    "                                    continue\n",
    "                        \n",
    "                        # Step 4: Concatenate all Parquet files for this asset in the year\n",
    "                        if parquet_files:\n",
    "                            combined_df = pl.concat(parquet_files, how=\"vertical\")\n",
    "                            \n",
    "                            combined_df = set_timeseries(combined_df)\n",
    "                            # Extract asset name (e.g., `EWW.P_bbo` from `EWW.P_bbo_2007.tar`)\n",
    "                            asset_name = member.name.rsplit(\"_\", 1)[0]\n",
    "                            \n",
    "                            # Append to the existing data for the same asset across years\n",
    "                            if asset_name in assets_data:\n",
    "                                assets_data[asset_name] = pl.concat([assets_data[asset_name], combined_df], how=\"vertical\")\n",
    "                            else:\n",
    "                                assets_data[asset_name] = combined_df\n",
    "                            print(f\"Combined DataFrame for {asset_name} now has {len(assets_data[asset_name])} rows.\")\n",
    "\n",
    "    # Step 5: Process or save the final combined DataFrames\n",
    "    for asset_name, df in assets_data.items():\n",
    "\n",
    "        print(f\"\\nFinal DataFrame for {asset_name}:\\n{df}\")\n",
    "        # Example: Save to disk if needed\n",
    "        # df.write_parquet(f\"Data/{asset_name}_combined.parquet\")\n",
    "\n",
    "    if assets_data:\n",
    "        k = sector\n",
    "        \n",
    "        df = assets_data[f\"./{k}.P_bbo\"]\n",
    "\n",
    "        df = df.with_columns(pl.col(\"datetime\").diff().alias(\"time_diff\"))\n",
    "\n",
    "        # Calculate the average difference\n",
    "        average_diff = df[\"time_diff\"].drop_nulls().mean()\n",
    "\n",
    "        print(f\"Average time difference for {k}: {average_diff}\")\n",
    "        average_diffs[k] = average_diff\n",
    "        del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_tickers = assets_tickers\n",
    "\n",
    "data = []\n",
    "for ticker in all_tickers:\n",
    "    if ticker in average_diffs:\n",
    "        seconds_val = average_diffs[ticker].total_seconds()  \n",
    "        data.append((ticker, seconds_val))\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Ticker\", \"Total_Seconds\"])\n",
    "\n",
    "df.to_csv(\"Data/average_diffs_seconds.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
