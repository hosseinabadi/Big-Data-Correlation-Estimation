{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltat = 5\n",
    "Countries_indexes = [\"EUSA\", \"EWC\", \"EWU\", \"EWG\", \"EWQ\", \"EWJ\", \"MCHI\", \"INDA\", \"EWA\", \"EWY\", \"EWW\", \"EWL\", \"EWT\", \"EWH\", \"EWS\", \"EWI\", \"EWP\", \"EWN\", \"EWD\", \"EWO\", \"EWK\", \"EDEN\", \"EFNL\", \"EIS\", \"EWZ\"]\n",
    "\n",
    "# List of yearly `.tar` files to process\n",
    "yearly_tar_files = [\n",
    "    # \"Data/ETFs/ETFs-2007.tar\",\n",
    "    #\"Data/ETFs/ETFs-2008.tar\",\n",
    "    \"Data/ETFs/ETFs-2009.tar\",\n",
    "    \"Data/ETFs/ETFs-2010.tar\",\n",
    "    \"Data/ETFs/ETFs-2011.tar\",\n",
    "    \"Data/ETFs/ETFs-2012.tar\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing yearly tar: Data/ETFs/ETFs-2009.tar\n",
      "Processing inner tar: ./INDA.P_bbo_2009.tar\n",
      "Processing inner tar: ./INDA.P_trade_2009.tar\n",
      "Processing yearly tar: Data/ETFs/ETFs-2010.tar\n",
      "Processing inner tar: ./INDA.P_bbo_2010.tar\n",
      "Processing inner tar: ./INDA.P_trade_2010.tar\n",
      "Processing yearly tar: Data/ETFs/ETFs-2011.tar\n",
      "Processing inner tar: ./INDA.P_bbo_2011.tar\n",
      "Processing inner tar: ./INDA.P_trade_2011.tar\n",
      "Processing yearly tar: Data/ETFs/ETFs-2012.tar\n",
      "Processing inner tar: ./INDA.P_bbo_2012.tar\n",
      "Combined DataFrame for ./INDA.P_bbo now has 335177 rows.\n",
      "Processing inner tar: ./INDA.P_trade_2012.tar\n",
      "\n",
      "Final DataFrame for ./INDA.P_bbo:\n",
      "shape: (335_177, 5)\n",
      "┌───────────┬────────────┬───────────┬────────────┬────────────────────────────────┐\n",
      "│ bid-price ┆ bid-volume ┆ ask-price ┆ ask-volume ┆ datetime                       │\n",
      "│ ---       ┆ ---        ┆ ---       ┆ ---        ┆ ---                            │\n",
      "│ f64       ┆ i32        ┆ f64       ┆ i32        ┆ datetime[μs, America/New_York] │\n",
      "╞═══════════╪════════════╪═══════════╪════════════╪════════════════════════════════╡\n",
      "│ 0.0       ┆ 0          ┆ 0.0       ┆ 0          ┆ 2012-02-03 03:35:05.467 EST    │\n",
      "│ 26.0      ┆ 2          ┆ 26.95     ┆ 2          ┆ 2012-02-03 14:44:32.109 EST    │\n",
      "│ 25.7      ┆ 10         ┆ 26.95     ┆ 2          ┆ 2012-02-03 16:12:05.995 EST    │\n",
      "│ 25.7      ┆ 10         ┆ 0.0       ┆ 0          ┆ 2012-02-03 16:12:06.724 EST    │\n",
      "│ 0.0       ┆ 0          ┆ 0.0       ┆ 0          ┆ 2012-02-03 16:12:06.993 EST    │\n",
      "│ …         ┆ …          ┆ …         ┆ …          ┆ …                              │\n",
      "│ 0.4       ┆ 1          ┆ 1016.12   ┆ 2          ┆ 2012-12-31 16:15:06.713 EST    │\n",
      "│ 0.4       ┆ 1          ┆ 1016.12   ┆ 1          ┆ 2012-12-31 16:15:06.713 EST    │\n",
      "│ 0.38      ┆ 2          ┆ 1016.12   ┆ 1          ┆ 2012-12-31 16:15:06.713 EST    │\n",
      "│ 0.38      ┆ 1          ┆ 1016.12   ┆ 1          ┆ 2012-12-31 16:15:06.713 EST    │\n",
      "│ 0.0       ┆ 0          ┆ 0.0       ┆ 0          ┆ 2012-12-31 16:15:06.722 EST    │\n",
      "└───────────┴────────────┴───────────┴────────────┴────────────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_18696\\24256094.py:22: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  (result[\"time-bucket\"].map_elements(lambda x: datetime.utcfromtimestamp(x))),\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from io import BytesIO\n",
    "import polars as pl\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_buckets(df, deltat = 5, only_trading_hours = True, opening_hour = \"10:00:00\", closing_hour = \"15:30:00\"):\n",
    "    date_list = df[\"datetime\"].dt.date().unique()\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (df[\"datetime\"].cast(float)/1e6 // deltat * deltat).alias('time-bucket')\n",
    "    )\n",
    "\n",
    "    result = df.group_by(\"time-bucket\").agg([\n",
    "        ((((pl.col(\"ask-price\")*pl.col(\"ask-volume\")).sum() / pl.col(\"ask-volume\").sum()) + \n",
    "         ((pl.col(\"bid-price\")*pl.col(\"bid-volume\")).sum() / pl.col(\"bid-volume\").sum())) / 2).alias(\"weighted-avg-price\")\n",
    "    ])\n",
    "    \n",
    "    \n",
    "\n",
    "    result = result.with_columns(\n",
    "        (result[\"time-bucket\"].map_elements(lambda x: datetime.utcfromtimestamp(x))),\n",
    "        result[\"weighted-avg-price\"].cast(float).alias(\"weighted-avg-price\")\n",
    "    )\n",
    "    result = result.with_columns(pl.col(\"time-bucket\").dt.convert_time_zone(\"America/New_York\"))\n",
    "    result = result.fill_nan(None)\n",
    "    result = result.sort(\"time-bucket\")\n",
    "\n",
    "    # Convert start and end dates to datetime\n",
    "    start_time = datetime.strptime(\"2007-01-01\", \"%Y-%m-%d\")\n",
    "    end_time = datetime.strptime(\"2012-12-31\", \"%Y-%m-%d\").replace(hour=23, minute=59, second=59)\n",
    "    # Generate time series with pl.date_range\n",
    "    time_series = pl.DataFrame({\n",
    "        \"time-bucket\": pl.datetime_range(\n",
    "            start=start_time,\n",
    "            end=end_time,\n",
    "            interval=f\"{deltat}s\",  # Step size in seconds\n",
    "            eager=True,\n",
    "            time_zone = \"America/New_York\"\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    result = result.join(time_series, on=\"time-bucket\", how=\"full\")\n",
    "    result = result.select([\n",
    "        pl.col(\"weighted-avg-price\").alias(\"weighted-avg-price\"),  \n",
    "        pl.col(\"time-bucket_right\").alias(\"time-bucket\") \n",
    "    ])\n",
    "    result = result.sort(\"time-bucket\")\n",
    "    result = result.select(pl.all().forward_fill())\n",
    "    result = result.drop_nulls()\n",
    "\n",
    "    if only_trading_hours:\n",
    "        hh_open,mm_open,ss_open = [float(x) for x in opening_hour.split(\":\")]\n",
    "        hh_close,mm_close,ss_close = [float(x) for x in closing_hour.split(\":\")]\n",
    "\n",
    "        seconds_open=hh_open*3600+mm_open*60+ss_open\n",
    "        seconds_close=hh_close*3600+mm_close*60+ss_close\n",
    "\n",
    "        result = result.filter(pl.col('time-bucket').dt.hour().cast(float)*3600+pl.col('time-bucket').dt.minute().cast(float)*60+pl.col('time-bucket').dt.second()>=seconds_open,\n",
    "                       pl.col('time-bucket').dt.hour().cast(float)*3600+pl.col('time-bucket').dt.minute().cast(float)*60+pl.col('time-bucket').dt.second()<=seconds_close)\n",
    "\n",
    "\n",
    "    result = result.filter(\n",
    "        pl.col(\"time-bucket\").dt.date().is_in(date_list)\n",
    "    )\n",
    "    return result.sort(\"time-bucket\")\n",
    "\n",
    "def remove_outliers(df, ticker = \"\"):\n",
    "    upper_bounds = {\"EDEN\": None, \"EFNL\": 900, \"EIS\": 100, \"EUSA\": {2010: 27.8, 2011:35}, \"EWA\": 900, \"EWC\": {2009:28, 2010: 900, 2011: 900, 2012: 900}, \"EWD\": 900, \"EWG\": 900, \"EWH\": 25, \"EWI\": {2011: 21.5}, \"EWJ\": 900, \"EWK\": 22.5, \"EWL\": 75, \"EWN\": 75, \"EWO\": 50, \"EWP\": 55, \"EWQ\": 29, \"EWS\": 20, \"EWT\": {2009: 14}, \"EWU\": {2009: 100, 2012: 20}, \"EWW\": 900, \"EWY\":900, \"EWZ\": 900, \"INDA\": {2012: 30}, \"MCHI\": 900}\n",
    "    lower_bounds = {\"EDEN\": None, \"EFNL\": -900, \"EIS\": -900, \"EUSA\": -900, \"EWA\": -900, \"EWC\": -900, \"EWD\": {2010: 13}, \"EWG\": {2009:13.2}, \"EWH\": -900, \"EWI\": -900, \"EWJ\": -900, \"EWK\": {2010: 9}, \"EWL\": {2011: 15}, \"EWN\": {2010: 15, 2011: 14}, \"EWO\": -900, \"EWP\": -900, \"EWQ\": -900, \"EWS\": {2011: 8}, \"EWT\": {2011: 9}, \"EWU\": -900, \"EWW\": -900, \"EWY\":-900, \"EWZ\": -900, \"INDA\": -900, \"MCHI\": -900} \n",
    "    # Define thresholds for each year\n",
    "    upper_bound = upper_bounds.get(ticker, 900)\n",
    "    lower_bound = lower_bounds.get(ticker, -900)\n",
    "\n",
    "    if upper_bound == None:\n",
    "        print(\"This data is problematic\")\n",
    "        upper_bound = 900\n",
    "        lower_bound = -900\n",
    "    if type(upper_bound) == int or type(upper_bound) == float:\n",
    "        upper_bound = {year: upper_bound for year in range(2009, 2013)}\n",
    "    if type(lower_bound) == int or type(lower_bound) == float:\n",
    "        lower_bound = {year: lower_bound for year in range(2009, 2013)}\n",
    "    \n",
    "\n",
    "    # Extract year from the time-bucket column\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"time-bucket\").dt.year().alias(\"year\")\n",
    "    )\n",
    "    \n",
    "    # Apply different thresholds for each year\n",
    "    result = df.with_columns(\n",
    "        pl.when((pl.col(\"weighted-avg-price\") > pl.col(\"year\").map_elements(lambda year: upper_bound.get(year, float('inf')))) | (pl.col(\"weighted-avg-price\") < pl.col(\"year\").map_elements(lambda year: lower_bound.get(year, float('-inf')))))\n",
    "        .then(None)\n",
    "        .otherwise(pl.col(\"weighted-avg-price\"))\n",
    "        .alias(\"weighted-avg-price\")\n",
    "    ).drop(\"year\")\n",
    "\n",
    "\n",
    "    result = result.select(pl.all().forward_fill())\n",
    "\n",
    "    return result.sort(\"time-bucket\")\n",
    "\n",
    "def set_timeseries(data):\n",
    "\n",
    "    base_date = datetime(1899, 12, 30)\n",
    "\n",
    "    data = data.with_columns(\n",
    "        (base_date + pl.col(\"xltime\").cast(float) * timedelta(days=1)).alias(\"datetime\")\n",
    "    )\n",
    "\n",
    "    data = data.with_columns(pl.col(\"datetime\").dt.convert_time_zone(\"America/New_York\"))\n",
    "    \n",
    "    data = data.drop(\"xltime\").sort(\"datetime\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Define the expected schemas\n",
    "schemas = {\n",
    "    \"bbo\": {\n",
    "        \"xltime\": pl.Float64,\n",
    "        \"bid-price\": pl.Float64,\n",
    "        \"bid-volume\": pl.Int32,\n",
    "        \"ask-price\": pl.Float64,\n",
    "        \"ask-volume\": pl.Int32,\n",
    "    },\n",
    "    \"trade\": {\n",
    "        \"xltime\": pl.Float64,\n",
    "        \"trade-price\": pl.Float64,\n",
    "        \"trade-volume\": pl.Int32,\n",
    "        \"trade-stringflag\": pl.Utf8,\n",
    "        \"trade-rawflag\": pl.Utf8,\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of yearly `.tar` files to process\n",
    "yearly_tar_files = [\n",
    "    # \"Data/ETFs/ETFs-2007.tar\",\n",
    "    #\"Data/ETFs/ETFs-2008.tar\",\n",
    "    \"Data/ETFs/ETFs-2009.tar\",\n",
    "    \"Data/ETFs/ETFs-2010.tar\",\n",
    "    \"Data/ETFs/ETFs-2011.tar\",\n",
    "    \"Data/ETFs/ETFs-2012.tar\",\n",
    "]\n",
    "\n",
    "# Dictionary to store concatenated DataFrames for each asset\n",
    "assets_data = {}\n",
    "\n",
    "def validate_and_fix_schema(df, expected_schema):\n",
    "    for col, expected_type in expected_schema.items():\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing column: {col}\")\n",
    "        \n",
    "        # If the column type does not match the expected type\n",
    "        if df[col].dtype != expected_type:\n",
    "            # Handle Float64 columns\n",
    "            if expected_type == pl.Float64:\n",
    "                df = df.with_columns(\n",
    "                    # Ensure the column is cast to Utf8 for string operations\n",
    "                    pl.when(pl.col(col).cast(pl.Utf8).str.strip_chars().is_in([\"\", \"()\", None]))\n",
    "                    .then(None)\n",
    "                    .otherwise(pl.col(col).cast(pl.Utf8))\n",
    "                    .str.replace_all(r\"[^\\d.]\", \"\")  # Remove non-numeric characters\n",
    "                    .cast(pl.Float64)  # Cast back to Float64\n",
    "                    .alias(col)\n",
    "                )\n",
    "            # Handle Int32 columns\n",
    "            elif expected_type == pl.Int32:\n",
    "                df = df.with_columns(\n",
    "                    # Ensure the column is cast to Utf8 for string operations\n",
    "                    pl.when(pl.col(col).cast(pl.Utf8).str.strip_chars().is_in([\"\", \"()\", None]))\n",
    "                    .then(None)\n",
    "                    .otherwise(pl.col(col).cast(pl.Utf8))\n",
    "                    .str.replace_all(r\"[^\\d]\", \"\")  # Remove non-numeric characters\n",
    "                    .cast(pl.Int32)  # Cast back to Int32\n",
    "                    .alias(col)\n",
    "                )\n",
    "            else:\n",
    "                # Cast other types directly\n",
    "                df = df.with_columns(df[col].cast(expected_type).alias(col))\n",
    "    return df\n",
    "\n",
    "for country in tqdm(Countries_indexes):\n",
    "    del assets_data\n",
    "    del schemas \n",
    "    schemas = {\n",
    "        \"bbo\": {\n",
    "            \"xltime\": pl.Float64,\n",
    "            \"bid-price\": pl.Float64,\n",
    "            \"bid-volume\": pl.Int32,\n",
    "            \"ask-price\": pl.Float64,\n",
    "            \"ask-volume\": pl.Int32,\n",
    "        },\n",
    "        \"trade\": {\n",
    "            \"xltime\": pl.Float64,\n",
    "            \"trade-price\": pl.Float64,\n",
    "            \"trade-volume\": pl.Int32,\n",
    "            \"trade-stringflag\": pl.Utf8,\n",
    "            \"trade-rawflag\": pl.Utf8,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    assets_data = {}\n",
    "    # Step 1: Iterate through yearly `.tar` files\n",
    "    for yearly_tar_path in yearly_tar_files:\n",
    "        print(f\"Processing yearly tar: {yearly_tar_path}\")\n",
    "        \n",
    "        with tarfile.open(yearly_tar_path, \"r\") as outer_tar:\n",
    "            # Step 2: Iterate through files in the yearly `.tar`\n",
    "            for member in outer_tar.getmembers():\n",
    "                if member.isfile() and member.name.startswith(f\"./{country}\") and member.name.endswith(\".tar\"):\n",
    "                    print(f\"Processing inner tar: {member.name}\")\n",
    "                    \n",
    "                    # Determine file type (\"bbo\" or \"trade\") based on the name\n",
    "                    if \"bbo\" in member.name:\n",
    "                        file_type = \"bbo\"\n",
    "                        expected_schema = schemas[\"bbo\"]\n",
    "                    elif \"trade\" in member.name:\n",
    "                        continue\n",
    "                        file_type = \"trade\"\n",
    "                        expected_schema = schemas[\"trade\"]\n",
    "                    else:\n",
    "                        print(f\"Skipping unknown file type: {member.name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Step 3: Extract the inner `.tar` file\n",
    "                    inner_tar_data = BytesIO(outer_tar.extractfile(member).read())\n",
    "                    with tarfile.open(fileobj=inner_tar_data, mode=\"r\") as inner_tar:\n",
    "                        parquet_files = []\n",
    "                        \n",
    "                        for inner_member in inner_tar.getmembers():\n",
    "                            # Look for `.parquet` files\n",
    "                            if inner_member.isfile() and inner_member.name.endswith(\".parquet\"):\n",
    "                                parquet_data = BytesIO(inner_tar.extractfile(inner_member).read())\n",
    "                                df = pl.read_parquet(parquet_data)\n",
    "                                \n",
    "                                # Validate and fix schema\n",
    "                                try:\n",
    "                                    df = validate_and_fix_schema(df, expected_schema)\n",
    "                                    parquet_files.append(df)\n",
    "                                except ValueError as e:\n",
    "                                    print(f\"Schema error in file {inner_member.name}: {e}\")\n",
    "                                    continue\n",
    "                        \n",
    "                        # Step 4: Concatenate all Parquet files for this asset in the year\n",
    "                        if parquet_files:\n",
    "                            combined_df = pl.concat(parquet_files, how=\"vertical\")\n",
    "                            \n",
    "                            combined_df = set_timeseries(combined_df)\n",
    "                            # Extract asset name (e.g., `EWW.P_bbo` from `EWW.P_bbo_2007.tar`)\n",
    "                            asset_name = member.name.rsplit(\"_\", 1)[0]\n",
    "                            \n",
    "                            # Append to the existing data for the same asset across years\n",
    "                            if asset_name in assets_data:\n",
    "                                assets_data[asset_name] = pl.concat([assets_data[asset_name], combined_df], how=\"vertical\")\n",
    "                            else:\n",
    "                                assets_data[asset_name] = combined_df\n",
    "                            print(f\"Combined DataFrame for {asset_name} now has {len(assets_data[asset_name])} rows.\")\n",
    "\n",
    "    # Step 5: Process or save the final combined DataFrames\n",
    "    for asset_name, df in assets_data.items():\n",
    "\n",
    "        print(f\"\\nFinal DataFrame for {asset_name}:\\n{df}\")\n",
    "        # Example: Save to disk if needed\n",
    "        # df.write_parquet(f\"Data/{asset_name}_combined.parquet\")\n",
    "    if assets_data:\n",
    "        df_new = get_buckets(assets_data[f\"./{country}.P_bbo\"], deltat = deltat, only_trading_hours = True)\n",
    "        df_new = remove_outliers(df_new, country)\n",
    "        df_new.write_parquet(f\"Data/clean_new/{country}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
